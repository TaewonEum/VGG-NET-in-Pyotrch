{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08a5277-34a7-4b75-8a75-58bd1f93fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "###VGGNET in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abdb681-debe-49e0-a512-6298d11dd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6735563c-6696-408c-8843-0e53a2c2903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGnet(nn.Module):\n",
    "    def __init__(self,modle,in_channel=3,num_class=10): \n",
    "        super(VGGnet,self).__init__()\n",
    "        \n",
    "        self.in_channels=in_channels\n",
    "        \n",
    "        #모델별 구조 딕셔너리로 define->convolution과 maxpooling 구조 반복하면서 이미지 사이즈는 줄여주고 채널수는 늘리면서 층을 깊게 쌓는 구조\n",
    "        self.VGG_types = {\n",
    "            'VGG11': [64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "            'VGG13': [64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "            'VGG16': [64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'],\n",
    "            'VGG19': [64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M']\n",
    "        }\n",
    "        \n",
    "        #create conv layer\n",
    "        self.conv_layer=self.create_conv_laters(self.VGG_types[model]) #model에 VGG11,13,16,19등 사용할 모델명을 삽입해줌. create_conv_laters는 따로 define한 함수\n",
    "        \n",
    "        #fully connected layer define\n",
    "        self.fcs=nn.Sequential(\n",
    "            nn.Linear(512*7*7,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024,num_class),\n",
    "        )\n",
    "        \n",
    "        def forward(self,x):\n",
    "            x=self.conv_layer(x)\n",
    "            x=x.view(-1,512*7*7)\n",
    "            x=self.fcs(x)\n",
    "            return x\n",
    "        \n",
    "        #define a function to create conv layer\n",
    "        def create_conv_laters(self,architecture): #filter 3x3 고정, stride=1x1 \n",
    "            layers = [] #빈리스트 생성\n",
    "            in_channels=self.in_channels #3\n",
    "            \n",
    "            for x in architecture: \n",
    "                if type(x) == int:\n",
    "                    out_channels = x #첫번째 블록 out channle 입력, self.VGG_types 리스트에서 하나씩->architecture=VGG_types list\n",
    "                    #nn. Conv2d (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
    "                    layers += [nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                               nn.BatchNorm2d(x),\n",
    "                               nn.ReLU()]\n",
    "                    in_channels=x #한번 끝나면 out_channels->in_channels로\n",
    "                elif x == 'M':\n",
    "                    layers += [nn.MaxPool2d(kernel_size(2,2), stride=(2,2))]\n",
    "                    \n",
    "            return nn.Sequential(*layers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ad6bf0-9999-40ab-8aa9-f2e990ea2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "###VGG16 Model Define VGG 11,13,16,19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dbea989-ae48-4493-9a81-e1fda4b7534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Model(nn.Module):\n",
    "    def __init__(self,train_dataset,val_dataset,num_classes,epoch,learning_rate,batch_size,loader_num_workers):\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_availabel() else 'cpu')\n",
    "        self.train_dataset=train_dataset\n",
    "        self.val_dataset=val_dataset\n",
    "        self.num_classes=num_classes\n",
    "        self.epoch=epoch\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size=batch_size\n",
    "        self.loader_num_workeres=loader_num_workers\n",
    "        self.patience_i= int(min(np.ceil(epoch*0.1), 10))  #이후에 lr scheduler parameter\n",
    "        \n",
    "    def VGG16Train(self):\n",
    "        model = VGGnet('VGG16', in_channels=3, num_classes=self.num_classes).to(self.device) # model gpu로 보내줌\n",
    "        \n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        optimizer=optim.Adam(model.parameters(),lr=self.learning_rate,weight_decay=1e-4)\n",
    "        lr_scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.patience_i,verbose=True) #val loss 정체되면 learning rate 조정\n",
    "        \n",
    "        train_loader=DataLoader(self.train_dataset,batch_size=self.batch_size, shuffle=True, num_workers=self.loader_num_workers) #train set\n",
    "        val_loader=DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False, num_workers=self.loader_num_workers) #val set\n",
    "        dataloaders={'train': train_loader, 'val':val_loader}\n",
    "        \n",
    "        since=time.time()\n",
    "        train_acc_history=[]\n",
    "        train_loss_history=[]\n",
    "        val_acc_history=[]\n",
    "        val_loss_history=[]\n",
    "        best_model_wts=copy.deepcopy(model.state_dict())\n",
    "        best_acc=0.0\n",
    "        use_auxiliary=False\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            epoch_time=time.time()\n",
    "            print('Epoch {}/{}'.format(i+1, self.epoch))\n",
    "            print('-'*10)\n",
    "            \n",
    "            for phase in ['train','val']:\n",
    "                if phase=='train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                running_loss=0.0\n",
    "                running_corrects=0\n",
    "                \n",
    "            for inputs,labels in dataloaders[phase]:\n",
    "                \n",
    "                inputs=inputs.to(self.device)\n",
    "                \n",
    "                labels=labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad() #한번 학습이 완료되면, gradient를 0으로 만들어주어야함, 초기화하지 않을시 의도한 방향과 다른방향으로 학습할 가능성\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    if phase=='train':\n",
    "                        if use_auxiliary:\n",
    "                            outputs, aux1, aux2 = model(inputs)\n",
    "                            loss = criterion(outputs,labels) + 0.3 *criterion(aux1,labels) +0.3*criterion(aux2,labels)\n",
    "                        else:\n",
    "                            outputs=model(inputs)\n",
    "                            loss=criterion(outputs,labels)\n",
    "                            \n",
    "                        _,preds = torch.max(outputs,1)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    if phase == 'val':\n",
    "                        outputs = models(inputs)\n",
    "                        loss=criterion(outputs,labels)\n",
    "                        _,preds=torch.max(outputs,1)\n",
    "                        \n",
    "                #통계값 출력 Part\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) #loss평균 계산 전체데이터\n",
    "            \n",
    "            if phase == 'val':\n",
    "                lr_scheduler.step(epochh_loss)\n",
    "                \n",
    "            epoch_acc = running_corrects.double()/len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            e_time = time.time() - epoch_time\n",
    "            \n",
    "            #deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                    val_acc_history.append(float(epoch_acc))\n",
    "                    val_loss_history.append(float(epoch_loss))\n",
    "            if phase == 'train':\n",
    "                    train_acc_history.append(float(epoch_acc))\n",
    "                    train_loss_history.append(float(epoch_loss))\n",
    "                    \n",
    "        print('Epoch Time : {:.0f}m {:.0f}s'.format(e_time//60, e_time%60))\n",
    "        print()\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        return model, self.epoch, train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f672c6e-fdb1-4b24-8739-fa0c77e72050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
